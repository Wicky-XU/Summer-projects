"""
COVID-19 è‚ºéƒ¨CTå›¾åƒåˆ†ç±»é¡¹ç›® - é¢„æµ‹å’Œè¯„ä¼°æ¨¡å—
==========================================

è´Ÿè´£æ¨¡å‹é¢„æµ‹ã€ç»“æœåˆ†æã€è¯„ä¼°æŒ‡æ ‡è®¡ç®—å’Œç»“æœä¿å­˜ç­‰åŠŸèƒ½
åŸºäºnotebookä¸­çš„é¢„æµ‹ç›¸å…³ä»£ç é‡æ„è€Œæˆ

"""

import os
import json
import numpy as np
import cv2
from pathlib import Path
from typing import Dict, List, Tuple, Any, Optional, Union
from tqdm import tqdm
import matplotlib.pyplot as plt
from datetime import datetime

import tensorflow as tf
from tensorflow.keras.models import load_model
from tensorflow.keras.applications.vgg16 import preprocess_input
from sklearn.metrics import classification_report, confusion_matrix

from config import get_config


class ModelPredictor:
    """æ¨¡å‹é¢„æµ‹å™¨"""
    
    def __init__(self, model_path: str = None, config=None):
        """
        åˆå§‹åŒ–é¢„æµ‹å™¨
        
        Args:
            model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
            config: é…ç½®å¯¹è±¡
        """
        self.config = config or get_config()
        self.model = None
        self.model_path = model_path
        self.class_names = {0: "no", 1: "yes"}  # é»˜è®¤ç±»åˆ«æ˜ å°„
        
        if model_path:
            self.load_model(model_path)
    
    def load_model(self, model_path: str) -> bool:
        """
        åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
        
        Args:
            model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
            
        Returns:
            bool: åŠ è½½æˆåŠŸè¿”å›True
        """
        try:
            print(f"ğŸ”„ åŠ è½½æ¨¡å‹: {model_path}")
            self.model = load_model(model_path)
            self.model_path = model_path
            print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
            
            # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
            print(f"ğŸ“Š æ¨¡å‹è¾“å…¥å½¢çŠ¶: {self.model.input_shape}")
            print(f"ğŸ“Š æ¨¡å‹è¾“å‡ºå½¢çŠ¶: {self.model.output_shape}")
            
            return True
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            return False
    
    def predict_single_image(self, image_path: str) -> Dict[str, Any]:
        """
        é¢„æµ‹å•å¼ å›¾åƒ
        
        Args:
            image_path: å›¾åƒæ–‡ä»¶è·¯å¾„
            
        Returns:
            é¢„æµ‹ç»“æœå­—å…¸
        """
        if self.model is None:
            raise ValueError("æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆè°ƒç”¨ load_model()")
        
        try:
            # åŠ è½½å’Œé¢„å¤„ç†å›¾åƒ
            img = cv2.imread(image_path)
            if img is None:
                raise ValueError(f"æ— æ³•è¯»å–å›¾åƒ: {image_path}")
            
            # é¢„å¤„ç†
            img_resized = cv2.resize(img, self.config.IMG_SIZE)
            img_preprocessed = preprocess_input(img_resized)
            img_batch = np.expand_dims(img_preprocessed, axis=0)
            
            # é¢„æµ‹
            prediction = self.model.predict(img_batch, verbose=0)[0][0]
            predicted_class = int(prediction > self.config.PREDICTION_THRESHOLD)
            confidence = prediction if predicted_class == 1 else 1 - prediction
            uncertainty = 1 - abs(prediction - 0.5) * 2
            
            return {
                'filename': os.path.basename(image_path),
                'path': image_path,
                'prediction': self.class_names[predicted_class],
                'confidence': float(confidence),
                'uncertainty': float(uncertainty),
                'raw_probability': float(prediction),
                'predicted_class': predicted_class
            }
            
        except Exception as e:
            print(f"âŒ é¢„æµ‹å›¾åƒå¤±è´¥ {image_path}: {str(e)}")
            return None
    
    def predict_batch_images(self, image_paths: List[str], 
                           batch_size: int = None) -> List[Dict[str, Any]]:
        """
        æ‰¹é‡é¢„æµ‹å›¾åƒ
        
        Args:
            image_paths: å›¾åƒè·¯å¾„åˆ—è¡¨
            batch_size: æ‰¹æ¬¡å¤§å°
            
        Returns:
            é¢„æµ‹ç»“æœåˆ—è¡¨
        """
        if self.model is None:
            raise ValueError("æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆè°ƒç”¨ load_model()")
        
        if batch_size is None:
            batch_size = self.config.BATCH_SIZE
        
        results = []
        valid_images = []
        valid_paths = []
        failed_images = []
        
        print(f"ğŸ”„ é¢„å¤„ç† {len(image_paths)} å¼ å›¾åƒ...")
        
        # é¢„å¤„ç†æ‰€æœ‰å›¾åƒ
        for img_path in tqdm(image_paths, desc="åŠ è½½å›¾åƒ"):
            try:
                img = cv2.imread(img_path)
                if img is not None:
                    img_resized = cv2.resize(img, self.config.IMG_SIZE)
                    img_preprocessed = preprocess_input(img_resized)
                    
                    # åŸºæœ¬è´¨é‡æ£€æŸ¥
                    if not np.allclose(img_preprocessed, 0):
                        valid_images.append(img_preprocessed)
                        valid_paths.append(img_path)
                    else:
                        failed_images.append((img_path, "å›¾åƒä¼¼ä¹å·²æŸå"))
                else:
                    failed_images.append((img_path, "æ— æ³•è¯»å–å›¾åƒ"))
            except Exception as e:
                failed_images.append((img_path, f"å¤„ç†é”™è¯¯: {str(e)}"))
        
        # æŠ¥å‘Šå¤±è´¥çš„å›¾åƒ
        if failed_images:
            print(f"âš ï¸ {len(failed_images)} å¼ å›¾åƒå¤„ç†å¤±è´¥")
            for path, reason in failed_images[:3]:
                print(f"  {os.path.basename(path)}: {reason}")
            if len(failed_images) > 3:
                print(f"  ... è¿˜æœ‰ {len(failed_images) - 3} ä¸ª")
        
        if not valid_images:
            print("âŒ æ²¡æœ‰æœ‰æ•ˆå›¾åƒç”¨äºé¢„æµ‹")
            return []
        
        print(f"ğŸ”„ å¼€å§‹é¢„æµ‹ {len(valid_images)} å¼ æœ‰æ•ˆå›¾åƒ...")
        
        # æ‰¹é‡é¢„æµ‹
        valid_images = np.array(valid_images)
        predictions = self.model.predict(valid_images, batch_size=batch_size, verbose=1)
        predicted_classes = (predictions > self.config.PREDICTION_THRESHOLD).astype(int).flatten()
        
        # å¤„ç†ç»“æœ
        for img_path, pred_prob, pred_class in zip(valid_paths, predictions.flatten(), predicted_classes):
            confidence = pred_prob if pred_class == 1 else 1 - pred_prob
            uncertainty = 1 - abs(pred_prob - 0.5) * 2
            
            results.append({
                'filename': os.path.basename(img_path),
                'path': img_path,
                'subdirectory': self._get_subdirectory(img_path),
                'prediction': self.class_names[pred_class],
                'confidence': float(confidence),
                'uncertainty': float(uncertainty),
                'raw_probability': float(pred_prob),
                'predicted_class': pred_class
            })
        
        print(f"âœ… é¢„æµ‹å®Œæˆï¼Œå¤„ç†äº† {len(results)} å¼ å›¾åƒ")
        return results
    
    def predict_test_directory(self, test_path: str = None) -> List[Dict[str, Any]]:
        """
        é¢„æµ‹æµ‹è¯•ç›®å½•ä¸­çš„æ‰€æœ‰å›¾åƒ
        
        Args:
            test_path: æµ‹è¯•ç›®å½•è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é…ç½®ä¸­çš„è·¯å¾„
            
        Returns:
            é¢„æµ‹ç»“æœåˆ—è¡¨
        """
        if test_path is None:
            test_path = str(self.config.TEST_PATH)
        
        print(f"ğŸ“‚ ä» {test_path} æ”¶é›†æµ‹è¯•å›¾åƒ...")
        
        # æ”¶é›†æ‰€æœ‰å›¾åƒè·¯å¾„
        image_paths = []
        for root, dirs, files in os.walk(test_path):
            for file in files:
                if any(file.lower().endswith(ext) for ext in self.config.SUPPORTED_IMAGE_FORMATS):
                    image_paths.append(os.path.join(root, file))
        
        if not image_paths:
            print("âŒ æµ‹è¯•ç›®å½•ä¸­æœªæ‰¾åˆ°å›¾åƒ")
            return []
        
        print(f"âœ… å‘ç° {len(image_paths)} å¼ æµ‹è¯•å›¾åƒ")
        
        # æŒ‰å­ç›®å½•åˆ†ç»„ç»Ÿè®¡
        subdir_stats = {}
        for path in image_paths:
            subdir = self._get_subdirectory(path)
            subdir_stats[subdir] = subdir_stats.get(subdir, 0) + 1
        
        print("ğŸ“Š å­ç›®å½•åˆ†å¸ƒ:")
        for subdir, count in subdir_stats.items():
            print(f"  {subdir}: {count} å¼ å›¾åƒ")
        
        # æ‰¹é‡é¢„æµ‹
        return self.predict_batch_images(image_paths)
    
    def _get_subdirectory(self, image_path: str) -> str:
        """è·å–å›¾åƒæ‰€åœ¨çš„å­ç›®å½•å"""
        path_parts = Path(image_path).parts
        test_path_parts = Path(self.config.TEST_PATH).parts
        
        if len(path_parts) > len(test_path_parts):
            relative_parts = path_parts[len(test_path_parts):]
            if len(relative_parts) > 1:
                return relative_parts[0]
        
        return "root"
    
    def set_class_mapping(self, class_mapping: Dict[int, str]) -> None:
        """
        è®¾ç½®ç±»åˆ«æ˜ å°„
        
        Args:
            class_mapping: ç±»åˆ«ç´¢å¼•åˆ°åç§°çš„æ˜ å°„
        """
        self.class_names = class_mapping
        print(f"âœ… ç±»åˆ«æ˜ å°„å·²æ›´æ–°: {self.class_names}")
    
    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        if self.model is None:
            return {}
        
        return {
            'model_path': self.model_path,
            'input_shape': str(self.model.input_shape),
            'output_shape': str(self.model.output_shape),
            'total_params': self.model.count_params(),
            'trainable_params': sum([tf.keras.backend.count_params(w) for w in self.model.trainable_weights])
        }


class ResultsAnalyzer:
    """ç»“æœåˆ†æå™¨"""
    
    def __init__(self, config=None):
        self.config = config or get_config()
    
    def analyze_predictions(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        åˆ†æé¢„æµ‹ç»“æœ
        
        Args:
            results: é¢„æµ‹ç»“æœåˆ—è¡¨
            
        Returns:
            åˆ†æç»“æœå­—å…¸
        """
        if not results:
            print("âŒ æ²¡æœ‰é¢„æµ‹ç»“æœå¯åˆ†æ")
            return {}
        
        print(f"ğŸ“Š åˆ†æ {len(results)} ä¸ªé¢„æµ‹ç»“æœ...")
        
        # åŸºæœ¬ç»Ÿè®¡
        total_predictions = len(results)
        class_counts = {}
        confidence_stats = {'yes': [], 'no': []}
        uncertainty_stats = {'yes': [], 'no': []}
        subdir_stats = {}
        
        for result in results:
            pred = result['prediction']
            subdir = result.get('subdirectory', 'unknown')
            
            # ç±»åˆ«ç»Ÿè®¡
            class_counts[pred] = class_counts.get(pred, 0) + 1
            confidence_stats[pred].append(result['confidence'])
            uncertainty_stats[pred].append(result['uncertainty'])
            
            # å­ç›®å½•ç»Ÿè®¡
            if subdir not in subdir_stats:
                subdir_stats[subdir] = {'yes': 0, 'no': 0, 'total': 0}
            subdir_stats[subdir][pred] += 1
            subdir_stats[subdir]['total'] += 1
        
        # ç½®ä¿¡åº¦åˆ†æ
        all_confidences = [r['confidence'] for r in results]
        all_uncertainties = [r['uncertainty'] for r in results]
        
        # ç½®ä¿¡åº¦ç­‰çº§åˆ†å¸ƒ
        confidence_levels = {
            'very_high': sum(1 for c in all_confidences if c > self.config.CONFIDENCE_LEVELS['very_high']),
            'high': sum(1 for c in all_confidences if self.config.CONFIDENCE_LEVELS['high'] <= c <= self.config.CONFIDENCE_LEVELS['very_high']),
            'medium': sum(1 for c in all_confidences if self.config.CONFIDENCE_LEVELS['medium'] <= c < self.config.CONFIDENCE_LEVELS['high']),
            'low': sum(1 for c in all_confidences if c < self.config.CONFIDENCE_LEVELS['medium'])
        }
        
        analysis = {
            'total_predictions': total_predictions,
            'class_distribution': class_counts,
            'confidence_statistics': {
                'overall': {
                    'mean': np.mean(all_confidences),
                    'median': np.median(all_confidences),
                    'std': np.std(all_confidences),
                    'min': np.min(all_confidences),
                    'max': np.max(all_confidences)
                },
                'by_class': {}
            },
            'uncertainty_statistics': {
                'overall': {
                    'mean': np.mean(all_uncertainties),
                    'median': np.median(all_uncertainties),
                    'std': np.std(all_uncertainties)
                },
                'by_class': {}
            },
            'confidence_levels': confidence_levels,
            'subdirectory_distribution': subdir_stats,
            'raw_uncertainties': all_uncertainties  # ç”¨äºåç»­åˆ†æ
        }
        
        # æŒ‰ç±»åˆ«çš„è¯¦ç»†ç»Ÿè®¡
        for class_name in ['yes', 'no']:
            if class_name in confidence_stats and confidence_stats[class_name]:
                analysis['confidence_statistics']['by_class'][class_name] = {
                    'mean': np.mean(confidence_stats[class_name]),
                    'std': np.std(confidence_stats[class_name]),
                    'count': len(confidence_stats[class_name])
                }
                
                analysis['uncertainty_statistics']['by_class'][class_name] = {
                    'mean': np.mean(uncertainty_stats[class_name]),
                    'std': np.std(uncertainty_stats[class_name])
                }
        
        return analysis
    
    def print_analysis_report(self, analysis: Dict[str, Any]) -> None:
        """æ‰“å°è¯¦ç»†çš„åˆ†ææŠ¥å‘Š"""
        if not analysis:
            print("âŒ æ— åˆ†æç»“æœå¯æ˜¾ç¤º")
            return
        
        print("\n" + "="*70)
        print("ğŸ”¬ é¢„æµ‹ç»“æœåˆ†ææŠ¥å‘Š")
        print("="*70)
        
        # åŸºæœ¬ç»Ÿè®¡
        total = analysis['total_predictions']
        print(f"\nğŸ“Š åŸºæœ¬ç»Ÿè®¡:")
        print(f"æ€»é¢„æµ‹æ•°: {total}")
        
        # ç±»åˆ«åˆ†å¸ƒ
        print(f"\nğŸ¯ ç±»åˆ«åˆ†å¸ƒ:")
        for class_name, count in analysis['class_distribution'].items():
            percentage = (count / total) * 100
            covid_status = "COVID-19 é˜³æ€§" if class_name == 'yes' else "COVID-19 é˜´æ€§"
            print(f"  {covid_status}: {count:3d} å¼ å›¾åƒ ({percentage:5.1f}%)")
        
        # ç½®ä¿¡åº¦ç»Ÿè®¡
        conf_stats = analysis['confidence_statistics']['overall']
        print(f"\nğŸ“ˆ æ•´ä½“ç½®ä¿¡åº¦ç»Ÿè®¡:")
        print(f"  å¹³å‡ç½®ä¿¡åº¦: {conf_stats['mean']:.4f}")
        print(f"  ä¸­ä½æ•°ç½®ä¿¡åº¦: {conf_stats['median']:.4f}")
        print(f"  æ ‡å‡†å·®: {conf_stats['std']:.4f}")
        print(f"  èŒƒå›´: {conf_stats['min']:.4f} - {conf_stats['max']:.4f}")
        
        # æŒ‰ç±»åˆ«çš„ç½®ä¿¡åº¦
        print(f"\nğŸ“Š æŒ‰ç±»åˆ«ç½®ä¿¡åº¦ç»Ÿè®¡:")
        for class_name, stats in analysis['confidence_statistics']['by_class'].items():
            covid_status = "COVID-19 é˜³æ€§" if class_name == 'yes' else "COVID-19 é˜´æ€§"
            print(f"  {covid_status}: å¹³å‡ {stats['mean']:.4f} (Â±{stats['std']:.4f}), {stats['count']} æ ·æœ¬")
        
        # ç½®ä¿¡åº¦ç­‰çº§åˆ†å¸ƒ
        print(f"\nğŸ¯ ç½®ä¿¡åº¦ç­‰çº§åˆ†å¸ƒ:")
        levels = analysis['confidence_levels']
        level_descriptions = {
            'very_high': 'æé«˜ (>0.95)',
            'high': 'é«˜ (0.9-0.95)',
            'medium': 'ä¸­ç­‰ (0.7-0.9)',
            'low': 'ä½ (<0.7)'
        }
        
        for level, count in levels.items():
            percentage = (count / total) * 100
            desc = level_descriptions.get(level, level)
            print(f"  {desc}: {count} å¼ å›¾åƒ ({percentage:.1f}%)")
        
        # ä¸ç¡®å®šæ€§åˆ†æ
        unc_stats = analysis['uncertainty_statistics']['overall']
        print(f"\nğŸ² ä¸ç¡®å®šæ€§åˆ†æ:")
        print(f"  å¹³å‡ä¸ç¡®å®šæ€§: {unc_stats['mean']:.4f}")
        print(f"  ä¸­ä½æ•°ä¸ç¡®å®šæ€§: {unc_stats['median']:.4f}")
        
        high_uncertainty = sum(1 for u in analysis.get('raw_uncertainties', []) if u > 0.8)
        if high_uncertainty > 0:
            print(f"  é«˜ä¸ç¡®å®šæ€§æ ·æœ¬ (>0.8): {high_uncertainty} å¼ ")
        
        # å­ç›®å½•åˆ†å¸ƒ
        if len(analysis['subdirectory_distribution']) > 1:
            print(f"\nğŸ“ å­ç›®å½•åˆ†å¸ƒ:")
            for subdir, stats in analysis['subdirectory_distribution'].items():
                if stats['total'] > 0:
                    yes_pct = (stats['yes'] / stats['total']) * 100
                    no_pct = (stats['no'] / stats['total']) * 100
                    print(f"  {subdir}: {stats['total']} å¼ å›¾åƒ")
                    print(f"    é˜³æ€§: {stats['yes']} ({yes_pct:.1f}%), é˜´æ€§: {stats['no']} ({no_pct:.1f}%)")
        
        # é¢„æµ‹è´¨é‡è¯„ä¼°
        print(f"\nğŸ“ˆ é¢„æµ‹è´¨é‡è¯„ä¼°:")
        high_conf_count = levels['very_high'] + levels['high']
        high_conf_percentage = (high_conf_count / total) * 100
        
        if high_conf_percentage > 80:
            print(f"  âœ… é¢„æµ‹è´¨é‡ä¼˜ç§€ - {high_conf_percentage:.1f}% é«˜ç½®ä¿¡åº¦é¢„æµ‹")
        elif high_conf_percentage > 60:
            print(f"  âœ… é¢„æµ‹è´¨é‡è‰¯å¥½ - {high_conf_percentage:.1f}% é«˜ç½®ä¿¡åº¦é¢„æµ‹")
        else:
            print(f"  âš ï¸ é¢„æµ‹è´¨é‡éœ€å…³æ³¨ - ä»…{high_conf_percentage:.1f}% é«˜ç½®ä¿¡åº¦é¢„æµ‹")
        
        print("="*70)
    
    def get_top_confident_predictions(self, results: List[Dict[str, Any]], 
                                    n: int = 10) -> List[Dict[str, Any]]:
        """è·å–ç½®ä¿¡åº¦æœ€é«˜çš„é¢„æµ‹ç»“æœ"""
        return sorted(results, key=lambda x: x['confidence'], reverse=True)[:n]
    
    def get_uncertain_predictions(self, results: List[Dict[str, Any]], 
                                threshold: float = 0.7) -> List[Dict[str, Any]]:
        """è·å–ä¸ç¡®å®šæ€§é«˜çš„é¢„æµ‹ç»“æœ"""
        return [r for r in results if r['uncertainty'] > threshold]
    
    def get_low_confidence_predictions(self, results: List[Dict[str, Any]], 
                                     threshold: float = 0.7) -> List[Dict[str, Any]]:
        """è·å–ä½ç½®ä¿¡åº¦çš„é¢„æµ‹ç»“æœ"""
        return [r for r in results if r['confidence'] < threshold]
    
    def generate_classification_report(self, true_labels: List[str], 
                                     predicted_labels: List[str]) -> str:
        """
        ç”Ÿæˆåˆ†ç±»æŠ¥å‘Šï¼ˆéœ€è¦çœŸå®æ ‡ç­¾ï¼‰
        
        Args:
            true_labels: çœŸå®æ ‡ç­¾åˆ—è¡¨
            predicted_labels: é¢„æµ‹æ ‡ç­¾åˆ—è¡¨
            
        Returns:
            åˆ†ç±»æŠ¥å‘Šå­—ç¬¦ä¸²
        """
        return classification_report(true_labels, predicted_labels, 
                                   target_names=['COVID-19 é˜´æ€§', 'COVID-19 é˜³æ€§'])


class ResultsSaver:
    """ç»“æœä¿å­˜å™¨"""
    
    def __init__(self, config=None):
        self.config = config or get_config()
    
    def save_predictions_json(self, results: List[Dict[str, Any]], 
                            filepath: str, analysis: Dict[str, Any] = None,
                            model_info: Dict[str, Any] = None) -> None:
        """
        ä¿å­˜é¢„æµ‹ç»“æœä¸ºJSONæ ¼å¼
        
        Args:
            results: é¢„æµ‹ç»“æœåˆ—è¡¨
            filepath: ä¿å­˜è·¯å¾„
            analysis: åˆ†æç»“æœ
            model_info: æ¨¡å‹ä¿¡æ¯
        """
        # å‡†å¤‡ä¿å­˜æ•°æ®
        save_data = {
            'metadata': {
                'total_predictions': len(results),
                'timestamp': self._get_timestamp(),
                'config_version': self.config.CURRENT_VERSION,
                'prediction_threshold': self.config.PREDICTION_THRESHOLD,
                'image_size': self.config.IMG_SIZE,
                'class_names': self.config.CLASS_NAMES
            },
            'predictions': results
        }
        
        # æ·»åŠ æ¨¡å‹ä¿¡æ¯
        if model_info:
            save_data['metadata']['model_info'] = model_info
        
        # æ·»åŠ åˆ†æç»“æœ
        if analysis:
            save_data['analysis'] = analysis
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        Path(filepath).parent.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜JSON
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(save_data, f, indent=2, ensure_ascii=False)
            print(f"âœ… é¢„æµ‹ç»“æœå·²ä¿å­˜: {filepath}")
        except Exception as e:
            print(f"âŒ ä¿å­˜é¢„æµ‹ç»“æœå¤±è´¥: {str(e)}")
    
    def save_analysis_report(self, analysis: Dict[str, Any], 
                           filepath: str) -> None:
        """
        ä¿å­˜åˆ†ææŠ¥å‘Šä¸ºæ–‡æœ¬æ ¼å¼
        
        Args:
            analysis: åˆ†æç»“æœ
            filepath: ä¿å­˜è·¯å¾„
        """
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        Path(filepath).parent.mkdir(parents=True, exist_ok=True)
        
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write("COVID-19 è‚ºéƒ¨CTåˆ†ç±» - é¢„æµ‹ç»“æœåˆ†ææŠ¥å‘Š\n")
                f.write("="*50 + "\n\n")
                f.write(f"ç”Ÿæˆæ—¶é—´: {self._get_timestamp()}\n")
                f.write(f"æ€»é¢„æµ‹æ•°: {analysis['total_predictions']}\n\n")
                
                # ç±»åˆ«åˆ†å¸ƒ
                f.write("ç±»åˆ«åˆ†å¸ƒ:\n")
                total = analysis['total_predictions']
                for class_name, count in analysis['class_distribution'].items():
                    percentage = (count / total) * 100
                    covid_status = "COVID-19 é˜³æ€§" if class_name == 'yes' else "COVID-19 é˜´æ€§"
                    f.write(f"  {covid_status}: {count} ({percentage:.1f}%)\n")
                
                # ç½®ä¿¡åº¦ç»Ÿè®¡
                conf_stats = analysis['confidence_statistics']['overall']
                f.write(f"\nç½®ä¿¡åº¦ç»Ÿè®¡:\n")
                f.write(f"  å¹³å‡: {conf_stats['mean']:.4f}\n")
                f.write(f"  ä¸­ä½æ•°: {conf_stats['median']:.4f}\n")
                f.write(f"  æ ‡å‡†å·®: {conf_stats['std']:.4f}\n")
                f.write(f"  èŒƒå›´: {conf_stats['min']:.4f} - {conf_stats['max']:.4f}\n")
                
                # ç½®ä¿¡åº¦ç­‰çº§
                f.write(f"\nç½®ä¿¡åº¦ç­‰çº§åˆ†å¸ƒ:\n")
                for level, count in analysis['confidence_levels'].items():
                    percentage = (count / total) * 100
                    f.write(f"  {level}: {count} ({percentage:.1f}%)\n")
                
                # ä¸ç¡®å®šæ€§ç»Ÿè®¡
                unc_stats = analysis['uncertainty_statistics']['overall']
                f.write(f"\nä¸ç¡®å®šæ€§ç»Ÿè®¡:\n")
                f.write(f"  å¹³å‡: {unc_stats['mean']:.4f}\n")
                f.write(f"  ä¸­ä½æ•°: {unc_stats['median']:.4f}\n")
                f.write(f"  æ ‡å‡†å·®: {unc_stats['std']:.4f}\n")
            
            print(f"âœ… åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {filepath}")
        except Exception as e:
            print(f"âŒ ä¿å­˜åˆ†ææŠ¥å‘Šå¤±è´¥: {str(e)}")
    
    def save_csv_summary(self, results: List[Dict[str, Any]], 
                        filepath: str) -> None:
        """
        ä¿å­˜é¢„æµ‹ç»“æœçš„CSVæ‘˜è¦
        
        Args:
            results: é¢„æµ‹ç»“æœåˆ—è¡¨
            filepath: ä¿å­˜è·¯å¾„
        """
        try:
            import pandas as pd
            
            # å‡†å¤‡CSVæ•°æ®
            csv_data = []
            for result in results:
                csv_data.append({
                    'filename': result['filename'],
                    'prediction': result['prediction'],
                    'confidence': result['confidence'],
                    'uncertainty': result['uncertainty'],
                    'raw_probability': result['raw_probability'],
                    'subdirectory': result.get('subdirectory', 'unknown')
                })
            
            # åˆ›å»ºDataFrameå¹¶ä¿å­˜
            df = pd.DataFrame(csv_data)
            df.to_csv(filepath, index=False, encoding='utf-8')
            print(f"âœ… CSVæ‘˜è¦å·²ä¿å­˜: {filepath}")
            
        except ImportError:
            print("âš ï¸ éœ€è¦pandasåº“æ¥ä¿å­˜CSVæ–‡ä»¶")
        except Exception as e:
            print(f"âŒ ä¿å­˜CSVæ‘˜è¦å¤±è´¥: {str(e)}")
    
    def _get_timestamp(self) -> str:
        """è·å–å½“å‰æ—¶é—´æˆ³"""
        return datetime.now().strftime("%Y-%m-%d %H:%M:%S")


class EnsemblePredictor:
    """é›†æˆé¢„æµ‹å™¨"""
    
    def __init__(self, model_paths: List[str], config=None):
        """
        åˆå§‹åŒ–é›†æˆé¢„æµ‹å™¨
        
        Args:
            model_paths: æ¨¡å‹è·¯å¾„åˆ—è¡¨
            config: é…ç½®å¯¹è±¡
        """
        self.config = config or get_config()
        self.models = []
        self.model_paths = model_paths
        
        self._load_models()
    
    def _load_models(self):
        """åŠ è½½æ‰€æœ‰æ¨¡å‹"""
        print(f"ğŸ”„ åŠ è½½ {len(self.model_paths)} ä¸ªæ¨¡å‹è¿›è¡Œé›†æˆ...")
        
        for i, model_path in enumerate(self.model_paths):
            try:
                model = load_model(model_path)
                self.models.append(model)
                print(f"  âœ… æ¨¡å‹ {i+1} åŠ è½½æˆåŠŸ: {Path(model_path).name}")
            except Exception as e:
                print(f"  âŒ æ¨¡å‹ {i+1} åŠ è½½å¤±è´¥: {str(e)}")
        
        if not self.models:
            raise RuntimeError("æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•æ¨¡å‹")
        
        print(f"âœ… æˆåŠŸåŠ è½½ {len(self.models)} ä¸ªæ¨¡å‹ç”¨äºé›†æˆé¢„æµ‹")
    
    def predict_ensemble(self, image_paths: List[str]) -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨é›†æˆæ–¹æ³•è¿›è¡Œé¢„æµ‹
        
        Args:
            image_paths: å›¾åƒè·¯å¾„åˆ—è¡¨
            
        Returns:
            é›†æˆé¢„æµ‹ç»“æœåˆ—è¡¨
        """
        if not self.models:
            raise RuntimeError("æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹")
        
        print(f"ğŸ”„ ä½¿ç”¨ {len(self.models)} ä¸ªæ¨¡å‹è¿›è¡Œé›†æˆé¢„æµ‹...")
        
        # é¢„å¤„ç†å›¾åƒ
        valid_images = []
        valid_paths = []
        
        for img_path in tqdm(image_paths, desc="é¢„å¤„ç†å›¾åƒ"):
            try:
                img = cv2.imread(img_path)
                if img is not None:
                    img_resized = cv2.resize(img, self.config.IMG_SIZE)
                    img_preprocessed = preprocess_input(img_resized)
                    valid_images.append(img_preprocessed)
                    valid_paths.append(img_path)
            except Exception as e:
                print(f"âš ï¸ è·³è¿‡å›¾åƒ {img_path}: {str(e)}")
        
        if not valid_images:
            return []
        
        valid_images = np.array(valid_images)
        
        # è·å–æ‰€æœ‰æ¨¡å‹çš„é¢„æµ‹
        all_predictions = []
        for i, model in enumerate(self.models):
            print(f"ğŸ”„ æ¨¡å‹ {i+1} é¢„æµ‹ä¸­...")
            preds = model.predict(valid_images, batch_size=self.config.BATCH_SIZE, verbose=0)
            all_predictions.append(preds.flatten())
        
        # é›†æˆé¢„æµ‹ç»“æœ
        results = []
        for j, img_path in enumerate(valid_paths):
            # è·å–æ‰€æœ‰æ¨¡å‹å¯¹è¿™å¼ å›¾åƒçš„é¢„æµ‹
            image_predictions = [pred[j] for pred in all_predictions]
            
            # è®¡ç®—é›†æˆç»“æœ
            ensemble_prob = np.mean(image_predictions)
            prob_std = np.std(image_predictions)
            predicted_class = int(ensemble_prob > self.config.PREDICTION_THRESHOLD)
            confidence = ensemble_prob if predicted_class == 1 else 1 - ensemble_prob
            
            # é›†æˆä¸ç¡®å®šæ€§ï¼ˆç»“åˆæ¨¡å‹é—´å·®å¼‚ï¼‰
            model_disagreement = prob_std
            prediction_uncertainty = 1 - abs(ensemble_prob - 0.5) * 2
            ensemble_uncertainty = max(prediction_uncertainty, model_disagreement)
            
            results.append({
                'filename': os.path.basename(img_path),
                'path': img_path,
                'prediction': 'yes' if predicted_class == 1 else 'no',
                'confidence': float(confidence),
                'uncertainty': float(ensemble_uncertainty),
                'raw_probability': float(ensemble_prob),
                'predicted_class': predicted_class,
                'model_predictions': [float(p) for p in image_predictions],
                'model_std': float(prob_std),
                'ensemble_method': 'mean'
            })
        
        print(f"âœ… é›†æˆé¢„æµ‹å®Œæˆï¼Œå¤„ç†äº† {len(results)} å¼ å›¾åƒ")
        return results


# ä¾¿æ·å‡½æ•°
def predict_and_analyze(model_path: str, test_path: str = None, 
                       save_results: bool = True, version: str = None) -> Tuple[List[Dict], Dict[str, Any]]:
    """
    å®Œæ•´çš„é¢„æµ‹å’Œåˆ†ææµç¨‹
    
    Args:
        model_path: æ¨¡å‹è·¯å¾„
        test_path: æµ‹è¯•æ•°æ®è·¯å¾„
        save_results: æ˜¯å¦ä¿å­˜ç»“æœ
        version: ç‰ˆæœ¬åç§°
        
    Returns:
        (é¢„æµ‹ç»“æœ, åˆ†æç»“æœ)
    """
    config = get_config()
    if version:
        config.switch_version(version)
    
    print("ğŸš€ å¼€å§‹å®Œæ•´çš„é¢„æµ‹å’Œåˆ†ææµç¨‹")
    print("="*50)
    
    # åˆ›å»ºé¢„æµ‹å™¨
    predictor = ModelPredictor(model_path, config)
    if not predictor.model:
        raise RuntimeError("æ¨¡å‹åŠ è½½å¤±è´¥")
    
    # æ‰§è¡Œé¢„æµ‹
    print("\nğŸ“Š æ‰§è¡Œé¢„æµ‹...")
    results = predictor.predict_test_directory(test_path)
    if not results:
        raise RuntimeError("é¢„æµ‹å¤±è´¥æˆ–æ— æµ‹è¯•æ•°æ®")
    
    # åˆ†æç»“æœ
    print("\nğŸ“ˆ åˆ†æé¢„æµ‹ç»“æœ...")
    analyzer = ResultsAnalyzer(config)
    analysis = analyzer.analyze_predictions(results)
    analyzer.print_analysis_report(analysis)
    
    # ä¿å­˜ç»“æœ
    if save_results:
        print("\nğŸ’¾ ä¿å­˜ç»“æœ...")
        saver = ResultsSaver(config)
        
        # ç”Ÿæˆæ–‡ä»¶è·¯å¾„
        version_str = version or config.CURRENT_VERSION
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        json_path = config.PREDICTIONS_PATH / f"detailed_predictions_{version_str}_{timestamp}.json"
        report_path = config.RESULTS_PATH / f"analysis_report_{version_str}_{timestamp}.txt"
        csv_path = config.PREDICTIONS_PATH / f"predictions_summary_{version_str}_{timestamp}.csv"
        
        # è·å–æ¨¡å‹ä¿¡æ¯
        model_info = predictor.get_model_info()
        
        # ä¿å­˜æ–‡ä»¶
        saver.save_predictions_json(results, str(json_path), analysis, model_info)
        saver.save_analysis_report(analysis, str(report_path))
        saver.save_csv_summary(results, str(csv_path))
        
        print(f"âœ… ç»“æœå·²ä¿å­˜:")
        print(f"  ğŸ“„ JSONè¯¦æƒ…: {json_path}")
        print(f"  ğŸ“„ åˆ†ææŠ¥å‘Š: {report_path}")
        print(f"  ğŸ“„ CSVæ‘˜è¦: {csv_path}")
    
    return results, analysis


def predict_with_ensemble(model_paths: List[str], test_path: str = None,
                         save_results: bool = True) -> Tuple[List[Dict], Dict[str, Any]]:
    """
    ä½¿ç”¨é›†æˆæ¨¡å‹è¿›è¡Œé¢„æµ‹å’Œåˆ†æ
    
    Args:
        model_paths: æ¨¡å‹è·¯å¾„åˆ—è¡¨
        test_path: æµ‹è¯•æ•°æ®è·¯å¾„
        save_results: æ˜¯å¦ä¿å­˜ç»“æœ
        
    Returns:
        (é¢„æµ‹ç»“æœ, åˆ†æç»“æœ)
    """
    config = get_config()
    
    print("ğŸ”— å¼€å§‹é›†æˆæ¨¡å‹é¢„æµ‹æµç¨‹")
    print("="*50)
    
    # æ”¶é›†æµ‹è¯•å›¾åƒ
    if test_path is None:
        test_path = str(config.TEST_PATH)
    
    image_paths = []
    for root, dirs, files in os.walk(test_path):
        for file in files:
            if any(file.lower().endswith(ext) for ext in config.SUPPORTED_IMAGE_FORMATS):
                image_paths.append(os.path.join(root, file))
    
    if not image_paths:
        raise RuntimeError("æœªæ‰¾åˆ°æµ‹è¯•å›¾åƒ")
    
    # åˆ›å»ºé›†æˆé¢„æµ‹å™¨
    ensemble = EnsemblePredictor(model_paths, config)
    
    # æ‰§è¡Œé›†æˆé¢„æµ‹
    results = ensemble.predict_ensemble(image_paths)
    if not results:
        raise RuntimeError("é›†æˆé¢„æµ‹å¤±è´¥")
    
    # åˆ†æç»“æœ
    analyzer = ResultsAnalyzer(config)
    analysis = analyzer.analyze_predictions(results)
    analyzer.print_analysis_report(analysis)
    
    # ä¿å­˜ç»“æœ
    if save_results:
        saver = ResultsSaver(config)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        json_path = config.PREDICTIONS_PATH / f"ensemble_predictions_{timestamp}.json"
        report_path = config.RESULTS_PATH / f"ensemble_analysis_{timestamp}.txt"
        
        # æ·»åŠ é›†æˆä¿¡æ¯
        ensemble_info = {
            'ensemble_method': 'mean',
            'model_count': len(model_paths),
            'model_paths': model_paths
        }
        
        saver.save_predictions_json(results, str(json_path), analysis, ensemble_info)
        saver.save_analysis_report(analysis, str(report_path))
    
    return results, analysis


def quick_predict(model_path: str, image_path: str) -> Dict[str, Any]:
    """
    å¿«é€Ÿé¢„æµ‹å•å¼ å›¾åƒ
    
    Args:
        model_path: æ¨¡å‹è·¯å¾„
        image_path: å›¾åƒè·¯å¾„
        
    Returns:
        é¢„æµ‹ç»“æœå­—å…¸
    """
    predictor = ModelPredictor(model_path)
    if not predictor.model:
        raise RuntimeError("æ¨¡å‹åŠ è½½å¤±è´¥")
    
    result = predictor.predict_single_image(image_path)
    if result:
        print(f"ğŸ“Š é¢„æµ‹ç»“æœ:")
        print(f"  æ–‡ä»¶: {result['filename']}")
        print(f"  é¢„æµ‹: {result['prediction']} ({'COVID-19 é˜³æ€§' if result['prediction'] == 'yes' else 'COVID-19 é˜´æ€§'})")
        print(f"  ç½®ä¿¡åº¦: {result['confidence']:.4f}")
        print(f"  ä¸ç¡®å®šæ€§: {result['uncertainty']:.4f}")
        
    return result


def compare_model_predictions(model_paths: List[str], test_path: str = None,
                            model_names: List[str] = None) -> Dict[str, List[Dict]]:
    """
    æ¯”è¾ƒå¤šä¸ªæ¨¡å‹çš„é¢„æµ‹ç»“æœ
    
    Args:
        model_paths: æ¨¡å‹è·¯å¾„åˆ—è¡¨
        test_path: æµ‹è¯•æ•°æ®è·¯å¾„
        model_names: æ¨¡å‹åç§°åˆ—è¡¨
        
    Returns:
        åŒ…å«å„æ¨¡å‹é¢„æµ‹ç»“æœçš„å­—å…¸
    """
    config = get_config()
    
    if model_names is None:
        model_names = [f"æ¨¡å‹{i+1}" for i in range(len(model_paths))]
    
    print(f"ğŸ”„ æ¯”è¾ƒ {len(model_paths)} ä¸ªæ¨¡å‹çš„é¢„æµ‹ç»“æœ")
    print("="*50)
    
    all_results = {}
    
    for model_path, model_name in zip(model_paths, model_names):
        print(f"\nğŸ“Š é¢„æµ‹ {model_name}...")
        try:
            predictor = ModelPredictor(model_path, config)
            results = predictor.predict_test_directory(test_path)
            all_results[model_name] = results
            print(f"âœ… {model_name} é¢„æµ‹å®Œæˆ: {len(results)} å¼ å›¾åƒ")
        except Exception as e:
            print(f"âŒ {model_name} é¢„æµ‹å¤±è´¥: {str(e)}")
            all_results[model_name] = []
    
    # åˆ†æä¸€è‡´æ€§
    if len(all_results) >= 2:
        print(f"\nğŸ“ˆ é¢„æµ‹ä¸€è‡´æ€§åˆ†æ:")
        _analyze_prediction_consistency(all_results)
    
    return all_results


def _analyze_prediction_consistency(all_results: Dict[str, List[Dict]]) -> None:
    """åˆ†æå¤šä¸ªæ¨¡å‹é¢„æµ‹ç»“æœçš„ä¸€è‡´æ€§"""
    model_names = list(all_results.keys())
    
    if len(model_names) < 2:
        return
    
    # æ‰¾åˆ°å…±åŒé¢„æµ‹çš„å›¾åƒ
    common_files = set()
    for results in all_results.values():
        if results:
            files = {r['filename'] for r in results}
            if not common_files:
                common_files = files
            else:
                common_files &= files
    
    if not common_files:
        print("  âŒ æ²¡æœ‰å…±åŒé¢„æµ‹çš„å›¾åƒ")
        return
    
    print(f"  ğŸ“Š å…±åŒé¢„æµ‹å›¾åƒæ•°: {len(common_files)}")
    
    # è®¡ç®—ä¸€è‡´æ€§
    agreements = 0
    disagreements = []
    
    for filename in common_files:
        predictions = []
        for model_name in model_names:
            for result in all_results[model_name]:
                if result['filename'] == filename:
                    predictions.append(result['prediction'])
                    break
        
        if len(set(predictions)) == 1:
            agreements += 1
        else:
            disagreements.append((filename, predictions))
    
    consistency_rate = agreements / len(common_files) * 100
    print(f"  ğŸ“ˆ é¢„æµ‹ä¸€è‡´æ€§: {consistency_rate:.1f}% ({agreements}/{len(common_files)})")
    
    if disagreements:
        print(f"  âš ï¸ ä¸ä¸€è‡´é¢„æµ‹æ•°: {len(disagreements)}")
        # æ˜¾ç¤ºå‰å‡ ä¸ªä¸ä¸€è‡´çš„æ¡ˆä¾‹
        for i, (filename, preds) in enumerate(disagreements[:3]):
            pred_str = ", ".join([f"{model_names[j]}:{p}" for j, p in enumerate(preds)])
            print(f"    {filename}: {pred_str}")
        if len(disagreements) > 3:
            print(f"    ... è¿˜æœ‰ {len(disagreements) - 3} ä¸ªä¸ä¸€è‡´æ¡ˆä¾‹")


# æµ‹è¯•å’Œæ¼”ç¤ºå‡½æ•°
def demo_prediction_pipeline():
    """æ¼”ç¤ºå®Œæ•´çš„é¢„æµ‹æµç¨‹"""
    print("ğŸ¬ COVID-19 é¢„æµ‹æ¨¡å—æ¼”ç¤º")
    print("="*50)
    
    config = get_config()
    
    # åˆ›å»ºæ¨¡æ‹Ÿç»“æœç”¨äºæ¼”ç¤º
    mock_results = [
        {
            'filename': 'covid_positive_001.jpg',
            'path': '/test/covid_positive_001.jpg',
            'subdirectory': 'test',
            'prediction': 'yes',
            'confidence': 0.95,
            'uncertainty': 0.1,
            'raw_probability': 0.95,
            'predicted_class': 1
        },
        {
            'filename': 'covid_negative_001.jpg',
            'path': '/test/covid_negative_001.jpg',
            'subdirectory': 'test',
            'prediction': 'no',
            'confidence': 0.88,
            'uncertainty': 0.24,
            'raw_probability': 0.12,
            'predicted_class': 0
        },
        {
            'filename': 'uncertain_case_001.jpg',
            'path': '/test/uncertain_case_001.jpg',
            'subdirectory': 'test',
            'prediction': 'yes',
            'confidence': 0.62,
            'uncertainty': 0.76,
            'raw_probability': 0.62,
            'predicted_class': 1
        }
    ]
    
    print("\nğŸ“Š æ¼”ç¤ºç»“æœåˆ†æ...")
    analyzer = ResultsAnalyzer(config)
    analysis = analyzer.analyze_predictions(mock_results)
    analyzer.print_analysis_report(analysis)
    
    print("\nğŸ’¾ æ¼”ç¤ºç»“æœä¿å­˜...")
    saver = ResultsSaver(config)
    
    # åˆ›å»ºä¸´æ—¶ä¿å­˜è·¯å¾„
    temp_json = config.PREDICTIONS_PATH / "demo_predictions.json"
    temp_report = config.RESULTS_PATH / "demo_report.txt"
    
    saver.save_predictions_json(mock_results, str(temp_json), analysis)
    saver.save_analysis_report(analysis, str(temp_report))
    
    print("\nâœ… æ¼”ç¤ºå®Œæˆ!")


if __name__ == "__main__":
    # è¿è¡Œæ¼”ç¤º
    demo_prediction_pipeline()